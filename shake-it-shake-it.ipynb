{"cells":[{"metadata":{"trusted":true,"_uuid":"a5c521e3734c5e64a27e390aebd9f0b717c15250"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nimport time\nimport os\nfrom keras import models\nfrom keras import layers\nfrom keras.utils import np_utils\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"print(\"Loading train data from CSV .....\")\ndf = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\nprint (\"Train data loaded\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de3cf815be33183610c29d45656d5d4ed0b7610b"},"cell_type":"code","source":"print (\"Processing train data to chunks\")\ndef process_chunk(chunk, rows, drop_col):\n    if drop_col ==True:\n        chunk = chunk.drop(columns='time_to_failure')\n    np_chunk = np.absolute(chunk.values)#.astype('int16')\n    np_chunk_means = np_chunk.reshape(-1,rows).mean(1).reshape(1,-1).transpose()\n    np_chunk_stds = np_chunk.reshape(-1,rows).std(1).reshape(1,-1).transpose()\n    np_chunk_medians = np.median(np_chunk.reshape(-1,rows), 1).reshape(1,-1).transpose()\n    np_chunk_averages = np.average(np_chunk.reshape(-1,rows), 1).reshape(1,-1).transpose()\n    np_chunk_amaxs = np.amax(np_chunk.reshape(-1,rows), 1).reshape(1,-1).transpose()\n    #np_chunk_sums = np.sum(np_chunk.transpose().reshape(-1,rows), 1).reshape(1,-1).transpose()\n    return np.concatenate((np_chunk_means, np_chunk_stds, np_chunk_medians, np_chunk_averages, np_chunk_amaxs), 1)\n\ndef to_chunks(test_split, ms):\n    \n    ttf_train = []\n    ttf_test = []\n    csv_size = len(df.index)\n    test_size = csv_size//(1/test_split)\n    train_size = csv_size - test_size\n    for m in range(ms*2):\n        if m < ms:            \n            imax = test_size//150000-1            \n        if m>= ms:\n            imax = train_size//150000-1            \n        imax = int(imax)\n        for i in range(imax):            \n            start=time.time()\n            if m < ms:            \n                row_start=i*150000+m*150000//ms\n            if m>= ms:            \n                row_start=test_size+i*150000+(m-ms)*150000//(ms)\n            row_start= int(row_start)\n            chunk = df.iloc[ row_start:row_start+150000, : ]\n            if m < ms:\n                ttf_test.append(chunk['time_to_failure'].values[::3000])#.reshape(1, 100))\n            if m>= ms:\n                ttf_train.append(chunk['time_to_failure'].values[::3000])#.reshape(1, 100))            \n            processed_chunk = process_chunk(chunk, 300, True)\n            processed_chunk = processed_chunk.reshape(1, processed_chunk.shape[0], processed_chunk.shape[1])\n\n            if i==0:\n                collector = processed_chunk\n            elif (i+1)%500 == 0 or i == imax-1:\n                collector=np.concatenate((collector, processed_chunk), 0)\n                if i<501 and m==0:\n                    x_test=collector\n                elif m<ms:\n                    x_test=np.concatenate((x_test, collector), 0)\n                \n                if i<501 and m==ms:\n                    x_train=collector\n                elif m>=ms:\n                    x_train=np.concatenate((x_train, collector), 0)\n                \n                print(\"i: \", i, \"|  m: \", m, \"|   execution time: \", ex_time)\n            elif i%500 == 0:\n                collector = processed_chunk\n            else:\n                collector=np.concatenate((collector, processed_chunk), 0)\n            end=time.time()\n            ex_time=end-start            \n            #, \"\\nx_train_shape:\", x_train.shape, \"|  labels_len\", len(ttf))\n    ttf_test = np.asarray(ttf_test)\n    ttf_train = np.asarray(ttf_train)\n    return x_test, ttf_test, x_train, ttf_train\nx_test, ttf_test, x_train, ttf_train = to_chunks(0.1 , 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2418fb4b361b0541d49974cd9ed222bfe1b6ba20"},"cell_type":"code","source":"model = models.Sequential()\nmodel.add(layers.Conv1D(16, 4, activation = 'relu', input_shape = (500, 5)))\nmodel.add(layers.Conv1D(32, 4, activation = 'relu'))\nmodel.add(layers.Conv1D(64, 4, activation = 'relu'))\nmodel.add(layers.MaxPooling1D(10))\nmodel.add(layers.Conv1D(128, 4, activation = 'relu'))\nmodel.add(layers.Conv1D(256, 4, activation = 'relu'))\nmodel.add(layers.Conv1D(512, 4, activation = 'relu'))\nmodel.add(layers.MaxPooling1D(10))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(1024, activation = 'relu'))\nmodel.add(layers.Dense(1024, activation = 'relu'))\nmodel.add(layers.Dense(50))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"debb8c03fc8ce7b755f74415ef5f1bc5fefc7b7a"},"cell_type":"code","source":"model.compile(optimizer='adam', loss='mae')\nmodel.fit(x_train, ttf_train, epochs = 10, batch_size=64, shuffle = True, validation_split = 0.1)\nprint(\"Test data loss:\")\nmodel.evaluate(x_test, ttf_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e28bbf6bb10b82da300ecc3420fb3e07e5bbb2ac"},"cell_type":"code","source":"print(x_test.shape, \n      ttf_test.shape, \n      x_train.shape, \n      ttf_train.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d033a9c5203e901797adb073fe3d67c3d530cdc9"},"cell_type":"code","source":"test_split=0.1\ncsv_size = len(df.index)\ntest_size = csv_size//(1/test_split)\ntrain_size = csv_size - test_size  \nm=5\nms = 5\ni=700\ni=0\nimax = train_size//150000-1            \nimax = int(imax)\n\nfor i in range(imax):\n    \n    row_start=test_size+i*150000+(m-ms)*150000//(ms)\n    row_start= int(row_start)\n    processed_chunk = process_chunk(df.iloc[row_start:row_start+150000, : ], 300, True)\n    processed_chunk = processed_chunk.reshape(1, processed_chunk.shape[0], processed_chunk.shape[1])\n    prediction = model.predict(processed_chunk)\n    if i == 0:\n        predict_full = prediction\n    else:\n        predict_full = np.concatenate((predict_full, prediction), 0)\n\na=predict_full.reshape(predict_full.shape[0]*predict_full.shape[1])[::50]\nb=ttf_train[:imax].reshape(ttf_train[:imax].shape[0]*ttf_train[:imax].shape[1])[::50]\nfig, ax1 = plt.subplots(figsize=(12, 8))\nplt.plot(b, color='b')\nplt.plot(a, \"ro\")\nax1.set_ylabel('ttf', color='r')\nplt.legend(['ttf'], loc=(0.01, 0.95))\nax2 = ax1.twinx()\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18d3c6ce64c4d79afec361f6299bbebc2cd7acb2"},"cell_type":"code","source":"#predictions_global = np.asarray(predictions).reshape(1,-1)\npredictions_global.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"cf45c7a9fc0caa4904747ac2c92d7b9536290a47"},"cell_type":"code","source":"segments_list = os.listdir('../input/test')\nsegments_list.sort()\npredictions=[]\nfor file_name in segments_list:\n    file = pd.read_csv('../input/test/'+file_name)\n    chunk = file.iloc[:, :]\n    processed_chunk = process_chunk(chunk, 300, False)\n    processed_chunk = processed_chunk.reshape(1, processed_chunk.shape[0], processed_chunk.shape[1])\n    predictions.append(model.predict(processed_chunk)[0][49])\n\npredictions_global = np.concatenate((np.asarray(predictions).reshape(1,-1), predictions_global), 0)    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3bd305c4ebb94cdfc9d45e4039e5fc5f79656ee"},"cell_type":"code","source":"predictions_global = np.mean(predictions_global, 0)\n\npredictions_global = predictions_global.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b6398a3d478ac72d8d5ddfd92df3870038b87e1"},"cell_type":"code","source":"len(predictions_global)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce9148acb67d65d2284b623b74a0506038e685b0"},"cell_type":"code","source":"to_submission = pd.DataFrame({'seg_id':[], 'time_to_failure':[]})\nto_submission.seg_id = segments_list\nto_submission.time_to_failure = predictions_global\nto_submission.seg_id = to_submission.seg_id.str.replace('.csv','', regex=False)\n#to_submission.to_csv('to_submission.csv', index=False)\n\n# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv(index = False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ncreate_download_link(to_submission)\n# create a link to download the dataframe\n##for i in range(1):\n  #  create_download_link(pd.DataFrame(x_train[:, :,i]), \"Download {} file\".format(i), \"xtrain{}\".format(i))\n    \n\n# ↓ ↓ ↓  Yay, download link! ↓ ↓ ↓","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59af329dfc62cdaa2f13a8166e8eb6345aa3847b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}